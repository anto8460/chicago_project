{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12),\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"end_to_end_project\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_police_data():\n",
    "    csv_path = os.path.join(os.path.join(\"police_data\"), \"crimes_2012_to_2017.csv\")\n",
    "    return pd.read_csv(csv_path, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and get basic info about shape and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "police_data = load_police_data()\n",
    "print(f'Shape of table: {police_data.shape}')\n",
    "# Show Unique value count for each column\n",
    "print(f'Unique values per column: \\n{police_data.nunique()}')\n",
    "police_data.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Data, drop/rename/add columns, convert to timeseries etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_attributes = ['Location',\n",
    "                   'ID',\n",
    "                   'Case Number',\n",
    "                   'IUCR',\n",
    "                   'Block',\n",
    "                   'Ward',\n",
    "                   'Primary Type',\n",
    "                   'Beat',\n",
    "                   'X Coordinate',\n",
    "                   'Community Area',\n",
    "                   'Year',\n",
    "                   'Latitude',\n",
    "                   'Longitude',\n",
    "                   'Y Coordinate',\n",
    "                   'Updated On',\n",
    "                   'Description',\n",
    "                   'Location Description']\n",
    "\n",
    "police_data = police_data.drop(columns=drop_attributes)\n",
    "\n",
    "# Rename columns\n",
    "police_data.rename(columns={\n",
    "    'Arrest': 'Arrest Status',\n",
    "    'Domestic': 'Domestic Status'\n",
    "}, inplace=True)\n",
    "\n",
    "# Convert to timeseries\n",
    "# A lot (X50) faster when we provide the format\n",
    "police_data['Date'] = pd.to_datetime(\n",
    "    police_data['Date'], format='%m/%d/%Y %I:%M:%S %p')\n",
    "\n",
    "# Set datetime as index, will not remove duplicates (verify_integrity=True would remove duplicates)\n",
    "police_data = police_data.set_index('Date')\n",
    "\n",
    "# Generate temp columns for plotting distributions\n",
    "police_data['Year'] = police_data.index.year\n",
    "police_data['Month'] = police_data.index.month\n",
    "police_data['Day of the Year'] = police_data.index.day_of_year\n",
    "\n",
    "# Drop rows with null values\n",
    "police_data= police_data.dropna()\n",
    "print(f'Shape of table: {police_data.shape}')\n",
    "\n",
    "# Show Unique value count for each column\n",
    "print(f'Unique values per column: \\n{police_data.nunique()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Categorical Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print bar diagrams for important columns\n",
    "columns_hist = [\n",
    "    'District',\n",
    "    'FBI Code',\n",
    "    'Arrest Status',\n",
    "    'Domestic Status',\n",
    "    'Year',\n",
    "    'Month',\n",
    "]\n",
    "\n",
    "for column in columns_hist:\n",
    "    police_data[column].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of crimes per {column}')\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Crime count')\n",
    "    plt.show()\n",
    "\n",
    "drop_attributes = [\n",
    "    'Year',\n",
    "    'Month',\n",
    "    'Day of the Year'\n",
    "]\n",
    "\n",
    "police_data = police_data.drop(columns=drop_attributes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeseries Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Use seaborn style defaults and set the default figure size\n",
    "sns.set(rc={'figure.figsize':(20, 10), 'lines.marker':'o', 'lines.linewidth': 0.5})\n",
    "# comment out to see all possible keys\n",
    "# mpl.rcParams.keys()\n",
    "\n",
    "# this helps with adding plot ticks on certain weekdays etc\n",
    "import matplotlib.dates as mdates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = police_data.resample('1W').count().plot(linestyle='None', legend=False)\n",
    "ax.set_ylabel('Weekly Crime count')\n",
    "ax.set_title('Weekly crime count across 5 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = police_data.loc['2015':'2016'].resample('1d').count().plot(linestyle='-', legend=False)\n",
    "ax.set_ylabel('Daily Crime count')\n",
    "ax.set_title('Daily crime count over the years 2015-2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = police_data.loc['2015/03':'2015/08'].resample('1d').count().plot(linestyle='-', legend=False)\n",
    "ax.set_ylabel('Daily Crime count')\n",
    "ax.set_title('Daily crime count over Spring Summer 2015')\n",
    "# Set x-axis major ticks to weekly interval, on Mondays\n",
    "ax.xaxis.set_major_locator(mdates.WeekdayLocator(byweekday=mdates.MONDAY))\n",
    "# Format x-tick labels as 3-letter month name and day number\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Integrated Moving Average Model\n",
    "\n",
    "- AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
    "- I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series\n",
    "  stationary.\n",
    "- MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns\n",
    "print(f'Shape of table: {police_data.shape}')\n",
    "drop_attributes = [\n",
    "    'Arrest Status',\n",
    "    'Domestic Status',\n",
    "    'District',\n",
    "    'FBI Code'\n",
    "]\n",
    "police_data = police_data.drop(columns=drop_attributes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create period table, with one column for crime count\n",
    "print(f'Shape of table before resampling: {police_data.shape}')\n",
    "\n",
    "police_data['Crime Count'] = \"\"\n",
    "# table with monthly period, skip 2017 data cause they are too little\n",
    "series = police_data.loc[:'2016/12'].resample(rule='1m', kind='period').count()\n",
    "print(f'Shape of period array: {series.shape}')\n",
    "print(series.info())\n",
    "series.tail(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Autocorrelation\n",
    "\n",
    "#### The parameters of the ARIMA model are defined as follows:\n",
    "\n",
    "- **p**: The number of lag observations included in the model, also called the lag order.\n",
    "- **d**: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
    "- **q**: The size of the moving average window, also called the order of moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "# This can help as choose the lag to be used in the Arima model, p value\n",
    "autocorrelation_plot(series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First try of training the ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pandas import DataFrame\n",
    "\n",
    "# fit model\n",
    "model = ARIMA(series, order=(5,1,0))\n",
    "model_fit = model.fit()\n",
    "# summary of fit model\n",
    "print(model_fit.summary())\n",
    "# line plot of residuals\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "plt.show()\n",
    "# density plot of residuals\n",
    "residuals.plot(kind='kde')\n",
    "plt.show()\n",
    "# summary stats of residuals\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split series in train and validation sets, then run a walk-forward train and validation - same hyperparameters as in previous step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def walk_forward_train_evaluate_and_report(series, arima_order):\n",
    "    # split into train and validation sets\n",
    "    X = series.values\n",
    "    size = int(len(X) * 0.66)\n",
    "    train, validation = X[0:size], X[size:len(X)]\n",
    "    history = [x for x in train]\n",
    "    predictions = list()\n",
    "    # walk-forward validation\n",
    "    for t in range(len(validation)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        # print('Fitting on:')\n",
    "        # print(history)\n",
    "        model_fit = model.fit()\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        obs = validation[t]\n",
    "        history.append(obs)\n",
    "        print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "    # evaluate forecasts\n",
    "    mse = mean_squared_error(validation, predictions)\n",
    "    print('validation MSE: %.3f' % mse)\n",
    "    # plot forecasts against actual outcomes\n",
    "    plt.plot(validation)\n",
    "    plt.plot(predictions, color='red')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "walk_forward_train_evaluate_and_report(series, (5, 1, 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA hyperparameters tuning with the Grid Search Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate an ARIMA model for a given order (p,d,q)\n",
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.66)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    error = mean_squared_error(test, predictions)\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supress errors noise\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate combinations of p, d and q values for an ARIMA model\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p, d, q)\n",
    "                try:\n",
    "                    mse = evaluate_arima_model(dataset, order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.3f' % (order, mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n",
    "    return best_cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training/validation set and final test set\n",
    "train_size = int(len(series) * 0.6)\n",
    "train_series, test_series = series[0:train_size], series[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate parameters\n",
    "p_values = [8, 10, 12]\n",
    "d_values = range(0, 3)\n",
    "q_values = range(0, 3)\n",
    "best_cfg = evaluate_models(train_series.values, p_values, d_values, q_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  We found the best hyperparameter combination, we can now evaluate the predictions against unseen before values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_forward_train_evaluate_and_report(test_series, best_cfg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80fe10d34dec841cc480c2013a4a6e49374e563a13184d67275abbe0d3d8cb02"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
